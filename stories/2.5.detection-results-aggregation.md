# Story 2.5: Detection Results Aggregation and Storage

**Status:** Ready for Development

---

## Story

**As a** Backend Developer,
**I want** to implement a system that aggregates detection results from multiple AI engines, stores them reliably, and provides efficient retrieval and analysis capabilities,
**so that** we can provide a unified detection results interface to clients and maintain a historical record for model improvement and reporting.

---

## Acceptance Criteria

1. Results aggregation logic implemented to combine outputs from damage, material, and volume detection engines
2. Unified detection response schema created for consistent client interface
3. Detection results stored in PostgreSQL database with proper schema and relationships
4. Results cached in Redis for fast retrieval with configurable TTL
5. Tag generation and assignment implemented based on detection results
6. User feedback/confirmation system implemented with correction tracking
7. Results search and filtering implemented by photo, detection type, tags, date range
8. Report generation capability implemented for insurance claims and delivery verification
9. Data versioning implemented to track detection result evolution
10. Detection results exported to S3 for archival and analytics

---

## Dev Notes

### Architecture Context
- **Service Type**: Python FastAPI service for results aggregation and metadata management [Source: architecture.md#4.7-Metadata-Service]
- **Database**: PostgreSQL with JSONB columns for flexible detection result storage [Source: architecture.md#4.7-Metadata-Service]
- **Cache**: Redis for fast result retrieval with pub/sub for real-time updates [Source: architecture.md#9.3-Optimization-Techniques]
- **Storage**: S3 for archiving detection results, segmentation masks, and depth maps [Source: architecture.md#6.1-Backend-Services]
- **Search**: Elasticsearch for full-text search on detection metadata (optional enhancement)

### Metadata Service Responsibilities
From architecture.md#4.7:
- Store and retrieve photo metadata
- Manage detection results and tags
- Handle user confirmations and corrections
- Provide search and filtering capabilities
- Support report generation

### Data Models
From architecture.md#4.7:
**Detections Table:**
```sql
detections:
  - id (UUID)
  - photo_id (UUID)
  - detection_type (ENUM: damage, material, volume)
  - model_version (TEXT)
  - results (JSONB)
  - confidence (FLOAT)
  - created_at (TIMESTAMP)
  - user_confirmed (BOOLEAN)
  - user_feedback (JSONB)
```

**Tags Table:**
```sql
tags:
  - id (UUID)
  - photo_id (UUID)
  - tag (TEXT)
  - source (ENUM: ai, user)
  - confidence (FLOAT)
```

### Results Aggregation Schema
**Unified Detection Response:**
```json
{
  "photo_id": "uuid",
  "detection_id": "uuid",
  "detected_at": "2025-11-18T12:00:00Z",
  "processing_time_ms": 1250,
  "model_versions": {
    "damage": "damage-v1.2.0",
    "material": "material-v1.1.0",
    "volume": "volume-v1.0.0"
  },
  "detections": {
    "damage": { },           # Damage detection results
    "material": { },         # Material detection results
    "volume": { }            # Volume estimation results
  },
  "aggregate_tags": [
    {
      "tag": "roof_damage",
      "source": "ai",
      "confidence": 0.92,
      "engines": ["damage"]
    },
    {
      "tag": "delivery_confirmation",
      "source": "ai",
      "confidence": 0.87,
      "engines": ["material"]
    }
  ],
  "summary": {
    "has_damage": true,
    "damage_severity": "moderate",
    "materials_detected": 2,
    "volume_estimated": true
  },
  "user_confirmation": {
    "status": "pending",  # pending, confirmed, corrected
    "confirmed_by": null,
    "confirmed_at": null,
    "corrections": null
  },
  "metadata": {
    "project_id": "uuid",
    "user_id": "uuid",
    "processing_region": "us-east-1",
    "api_version": "v1"
  }
}
```

### Technical Implementation Details

**Results Aggregation Pipeline**: [Source: architecture.md#5.1-Photo-Upload-&-Detection-Flow]
1. Receive results from AI Orchestrator for each detection type
2. Validate each result against its schema
3. Combine results into unified format
4. Generate automatic tags based on detections
5. Store in database with proper relationships
6. Cache in Redis for fast retrieval
7. Archive to S3 for long-term storage
8. Publish results to clients via WebSocket (real-time)

**Tag Generation Logic**:
- Damage detection → tags: roof_damage, hail_impact, wind_damage, missing_shingles, insurance_claim, [severity]
- Material detection → tags: delivery_confirmation, [material_types], [brands], quantity_alert (if variance)
- Volume estimation → tags: volume_estimated, [material_type], requires_confirmation (if low confidence)
- Cross-detection tags: potential_claim, multi_detection, high_confidence

**User Feedback System**: [Source: architecture.md#5-Data-Flow]
- Store user confirmations (accept/reject detections)
- Track corrections (adjusted counts, corrected damage type)
- Record user comments/notes
- Calculate feedback statistics per model version
- Use feedback for model retraining

**Caching Strategy**: [Source: architecture.md#9.3-Optimization-Techniques]
- L1: Detection results in Redis (TTL: 24 hours for active results, 1 hour for older)
- L2: Aggregated results cache (TTL: 12 hours)
- Cache invalidation on user feedback
- Pre-cache popular detection filters

**Search and Filtering**: [Source: architecture.md#4.7-Metadata-Service]
- Filter by detection type, date range, tags, confidence threshold
- Search by photo ID, project ID, user ID
- Advanced filters: damage type, material type, confidence range
- Pagination support for result sets
- Sorting: by detection_time, confidence, severity

**Report Generation**: [Source: architecture.md#7.2-Core-Endpoints]
Supported reports:
- Insurance Claims Report: Damage detections with photos and severity
- Delivery Verification Report: Material counts with photos
- Project Summary: All detections for a project with statistics
- Model Performance Report: Detection accuracy metrics

**File Structure**
```
backend/src/
├── services/
│   ├── results_aggregation_service.py # Results aggregation logic
│   ├── detection_storage_service.py    # Database operations
│   ├── results_cache_service.py        # Redis caching
│   └── tags_service.py                 # Tag generation and management
├── models/
│   ├── detection.py                    # Detection ORM model
│   ├── tags.py                         # Tags ORM model
│   └── user_feedback.py                # User feedback ORM model
├── schemas/
│   ├── detection_result_schema.py      # Response schemas
│   ├── aggregated_result_schema.py     # Aggregation schema
│   └── feedback_schema.py              # Feedback schemas
├── api/
│   ├── detection_routes.py             # Detection retrieval endpoints
│   ├── feedback_routes.py              # Feedback endpoints
│   └── report_routes.py                # Report generation endpoints
└── reports/
    ├── insurance_report_generator.py   # Insurance claim reports
    └── delivery_report_generator.py    # Delivery verification reports

backend/tests/
├── test_results_aggregation.py         # Aggregation logic tests
├── test_detection_storage.py           # Database tests
├── test_results_cache.py               # Cache tests
├── test_tags_generation.py             # Tag generation tests
└── test_report_generation.py           # Report generation tests

backend/alembic/versions/
└── *_create_detection_results_tables.py # Database migration
```

### Database Migrations
Create Alembic migrations for:
- Detections table with proper indexes
- Tags table with relationships
- User feedback table
- Detection version history table (for data versioning)
- Indexes on commonly queried columns: photo_id, created_at, user_id, project_id

### Dependencies
- SQLAlchemy >= 2.0.0 for ORM
- psycopg2-binary for PostgreSQL
- redis >= 4.0.0 for caching
- boto3 for S3 operations
- pydantic >= 2.0.0 for validation
- fastapi >= 0.95.0 for API
- pytest for testing
- pytest-asyncio for async tests

### Integration Points
- **Input**: AI Orchestrator (results from detection engines)
- **Output**: Detection API endpoints and WebSocket for real-time updates
- **Database**: PostgreSQL for persistent storage
- **Cache**: Redis for result caching and pub/sub
- **Storage**: S3 for archival
- **Clients**: Mobile/Web apps via REST API and WebSocket

### Performance Targets
- **Result Storage**: <100ms to store aggregated result in DB [Source: architecture.md#9.1-Performance-Targets]
- **Result Retrieval**: <200ms for cached results, <500ms for DB queries
- **Search**: <1s for complex filters across 1M+ photos
- **Report Generation**: <5s for typical project reports

### Testing Standards
- **Test Framework**: pytest with PostgreSQL test database fixtures
- **Unit Tests**: Aggregation logic, tag generation, validation (>85% coverage)
- **Integration Tests**: End-to-end result storage and retrieval
- **Database Tests**: Verify schema, relationships, indexes, migrations
- **Performance Tests**: Query optimization, caching effectiveness
- **Test Locations**: `/backend/tests/test_results_*.py`
- **Test Database**: Use Docker PostgreSQL container for tests
- **Fixtures**: Sample detection results from all three engines
- **Test Execution**: `pytest backend/tests/test_results_*.py -v --cov`

---

## Tasks / Subtasks

- [ ] Design and implement database schema for detection results (AC: 3, 9)
  - [ ] Create detections table with proper relationships
  - [ ] Create tags table with source tracking
  - [ ] Create user_feedback table for confirmations/corrections
  - [ ] Create detection_result_history table for versioning
  - [ ] Add proper indexes on frequently queried columns
  - [ ] Create Alembic migration for schema
  - [ ] Unit tests for schema validation

- [ ] Implement results aggregation logic (AC: 1, 2)
  - [ ] Create aggregation service for combining detection results
  - [ ] Implement damage detection result aggregation
  - [ ] Implement material detection result aggregation
  - [ ] Implement volume estimation result aggregation
  - [ ] Create unified response schema builder
  - [ ] Implement result validation
  - [ ] Unit tests for aggregation logic

- [ ] Implement detection results storage (AC: 3, 10)
  - [ ] Create database service for storing results
  - [ ] Implement transaction handling for atomicity
  - [ ] Create result versioning logic
  - [ ] Implement archival to S3
  - [ ] Add metadata tracking (processing region, etc.)
  - [ ] Unit tests for storage operations

- [ ] Implement Redis caching layer (AC: 4)
  - [ ] Create Redis cache service
  - [ ] Implement cache key generation strategy
  - [ ] Configure TTL for different result types
  - [ ] Implement cache invalidation on feedback
  - [ ] Add cache statistics/monitoring
  - [ ] Unit tests for caching

- [ ] Implement automatic tag generation (AC: 5)
  - [ ] Create tag generation service
  - [ ] Implement damage detection tag rules
  - [ ] Implement material detection tag rules
  - [ ] Implement volume estimation tag rules
  - [ ] Implement cross-engine tag combinations
  - [ ] Add confidence scoring for tags
  - [ ] Unit tests for tag generation

- [ ] Implement user feedback system (AC: 6)
  - [ ] Create feedback submission endpoint
  - [ ] Implement confirmation tracking (accept/reject)
  - [ ] Implement correction tracking (adjusted values)
  - [ ] Create feedback validation
  - [ ] Store feedback in database
  - [ ] Implement feedback statistics calculation
  - [ ] Unit tests for feedback system

- [ ] Implement results search and filtering (AC: 7)
  - [ ] Create search service with multiple filters
  - [ ] Implement detection type filtering
  - [ ] Implement date range filtering
  - [ ] Implement tag-based filtering
  - [ ] Implement confidence threshold filtering
  - [ ] Create pagination logic
  - [ ] Add sorting options (date, confidence, severity)
  - [ ] Unit tests for search operations

- [ ] Implement report generation (AC: 8)
  - [ ] Create insurance claim report generator
  - [ ] Create delivery verification report generator
  - [ ] Create project summary report generator
  - [ ] Create model performance report (if data available)
  - [ ] Implement PDF/JSON export formats
  - [ ] Add report formatting and styling
  - [ ] Unit tests for report generation

- [ ] Implement data versioning system (AC: 9)
  - [ ] Create version tracking for detection results
  - [ ] Implement version history queries
  - [ ] Add ability to retrieve previous versions
  - [ ] Create version comparison logic
  - [ ] Track what changed between versions
  - [ ] Unit tests for versioning

- [ ] Implement S3 archival system (AC: 10)
  - [ ] Create S3 archival service
  - [ ] Implement archival of detection results
  - [ ] Implement archival of segmentation masks
  - [ ] Implement archival of depth maps
  - [ ] Create archival scheduling
  - [ ] Implement retrieval from archive
  - [ ] Unit tests for archival

- [ ] Create API endpoints for result retrieval
  - [ ] Implement GET /api/v1/detections/:id
  - [ ] Implement GET /api/v1/photos/:id/detections
  - [ ] Implement POST /api/v1/detections/search (with filters)
  - [ ] Implement GET /api/v1/detections/:id/feedback
  - [ ] Implement POST /api/v1/detections/:id/feedback
  - [ ] Add error handling and validation
  - [ ] Integration tests for API endpoints

- [ ] Create report generation endpoints
  - [ ] Implement POST /api/v1/reports/insurance-claim
  - [ ] Implement POST /api/v1/reports/delivery-verification
  - [ ] Implement POST /api/v1/reports/project-summary
  - [ ] Add report formatting options
  - [ ] Integration tests for report endpoints

- [ ] Implement WebSocket support for real-time updates
  - [ ] Create WebSocket endpoint for detection updates
  - [ ] Implement result streaming to clients
  - [ ] Add subscription management
  - [ ] Implement pub/sub via Redis
  - [ ] Integration tests for WebSocket

- [ ] Create comprehensive database tests
  - [ ] Test schema integrity
  - [ ] Test relationships and foreign keys
  - [ ] Test indexes and query performance
  - [ ] Test migrations (up and down)
  - [ ] Test transaction handling
  - [ ] Test data consistency

- [ ] Implement monitoring and metrics
  - [ ] Create Prometheus metrics for storage latency
  - [ ] Track cache hit rates
  - [ ] Monitor query performance
  - [ ] Track feedback statistics
  - [ ] Create database size monitoring
  - [ ] Add archival rate metrics

- [ ] Create comprehensive unit and integration tests (>85% coverage)
  - [ ] Unit tests for all services
  - [ ] Integration tests for database operations
  - [ ] Integration tests for caching
  - [ ] Integration tests for S3 archival
  - [ ] Performance tests for queries
  - [ ] End-to-end tests for result flow

- [ ] Documentation and guides
  - [ ] Database schema documentation
  - [ ] API documentation for result endpoints
  - [ ] Report generation guide
  - [ ] Caching strategy documentation
  - [ ] Data archival and retention policies
  - [ ] Query optimization guide

---

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-11-18 | 1.0 | Initial story creation for Detection Results Aggregation | Scrum Master |

---
