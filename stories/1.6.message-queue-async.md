# Story 1.6: Message Queue and Async Processing Setup

**Status:** Ready for Development

---

## Story

**As a** System Architect,
**I want** to establish asynchronous message queue infrastructure and worker processes,
**so that** the detection pipeline can process photos without blocking API responses, handle failures gracefully, and scale processing independently from request handling.

---

## Acceptance Criteria

1. AWS SQS (or RabbitMQ for development) message queue created with proper queue configuration
2. Queue has separate priority levels: high-priority, normal-priority, low-priority queues
3. Message processing with retry logic: 3 retries with exponential backoff (1s, 2s, 4s delays)
4. Dead Letter Queue (DLQ) configured to capture messages failing after max retries
5. Worker service (Python/Node.js) processes messages from queue asynchronously
6. Worker tracks processing state: received, processing, completed, failed with status updates to database
7. Message schema defined and validated for photo processing requests
8. Monitoring and alerts configured for queue depth, processing lag, and DLQ messages

---

## Dev Notes

### Message Queue Architecture
- **Queue Technology**: AWS SQS (production) with RabbitMQ option for development [Source: architecture.md#6.1-Backend-Services]
- **Purpose**: Decouple photo upload API from long-running detection processing [Source: architecture.md#5.1-Photo-Upload-and-Detection-Flow]
- **Message Type**: Async photo processing requests with routing to appropriate detection engines
- **Scaling**: Independent worker scale-up/down based on queue depth without affecting API service

### AWS SQS Queue Configuration
- **Queue Names**:
  - `companycam-photos-high-priority-{environment}`
  - `companycam-photos-normal-priority-{environment}`
  - `companycam-photos-low-priority-{environment}`
- **Message Retention**: 14 days (standard SQS)
- **Visibility Timeout**: 300 seconds (5 minutes for processing)
- **Message Size**: 256 KB (sufficient for detection requests with metadata)
- **Long Polling**: Enabled (20-second wait time to reduce API calls)

### Message Schema
```json
{
  "message_id": "uuid",
  "photo_id": "uuid",
  "user_id": "uuid",
  "project_id": "uuid",
  "s3_url": "https://companycam-photos.s3.amazonaws.com/...",
  "s3_key": "project_id/year/month/day/photo.jpg",
  "detection_types": ["damage", "material"],
  "priority": "normal",
  "metadata": {
    "file_size": 2097152,
    "dimensions": {"width": 4000, "height": 3000},
    "timestamp": "2025-11-17T10:30:00Z"
  },
  "created_at": "2025-11-17T10:30:00Z"
}
```

### Retry and Backoff Strategy
- **Attempt 1**: Immediate (at message receipt)
- **Attempt 2**: Retry after 1 second
- **Attempt 3**: Retry after 2 seconds
- **Attempt 4**: Retry after 4 seconds
- **After 4 attempts**: Send to Dead Letter Queue
- **Exponential Backoff**: Use jitter to prevent thundering herd
- **DLQ Retention**: 14 days for analysis and manual reprocessing

### Worker Service Architecture
- **Technology**: Python FastAPI/Celery or Node.js with Bull queue
- **Responsibilities**:
  - Consume messages from queue
  - Parse and validate message schema
  - Update database with processing status
  - Publish to next processing step (detection services)
  - Handle errors and retry logic
  - Send completion notifications
- **Concurrency**: Configurable worker threads (default: 4 workers × 10 threads = 40 concurrent)

### Processing State Lifecycle
```
queued → received → processing → completed/failed
```
- `queued`: Message in SQS queue
- `received`: Worker picked up message, processing started
- `processing`: Detection pipeline running, timestamp tracked
- `completed`: Detection results available, status update sent to database
- `failed`: Processing failed after retries, message sent to DLQ, error logged

### Worker Status Database Schema
```sql
CREATE TABLE processing_jobs (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  photo_id UUID NOT NULL,
  queue_name VARCHAR(100),
  message_id VARCHAR(255),
  status VARCHAR(50),  -- queued, received, processing, completed, failed
  started_at TIMESTAMP,
  completed_at TIMESTAMP,
  error_message TEXT,
  retry_count INTEGER DEFAULT 0,
  processing_time_ms INTEGER,
  created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
  FOREIGN KEY (photo_id) REFERENCES photos(id),
  INDEX processing_jobs_photo_id_idx ON (photo_id),
  INDEX processing_jobs_status_idx ON (status)
);
```

### RabbitMQ Configuration (Development)
- **Containers**: RabbitMQ image in docker-compose.yml
- **Default Credentials**: guest/guest (development only)
- **Queue Settings**:
  - Durable queues for persistence
  - Message TTL: 14 days
  - Dead letter exchange for failed messages
- **Web Management**: RabbitMQ management plugin on port 15672

### Monitoring and Alerting
- **CloudWatch Metrics**:
  - Queue depth (ApproximateNumberOfMessages)
  - Processing time (custom metric from worker)
  - Failed messages (ApproximateNumberOfMessagesNotVisible + DLQ count)
  - Retry rate
- **Alerts**:
  - Queue depth > 1000 (trigger auto-scaling)
  - DLQ messages > 10 (manual intervention needed)
  - Processing time > 5 minutes (possible failure)
- **Dashboard**: Grafana dashboard showing queue health and processing metrics

### Error Handling Strategy
- **Transient Errors** (retry):
  - Network timeouts
  - S3 connection errors
  - Database connection errors
  - API gateway timeouts
- **Permanent Errors** (DLQ immediately):
  - Invalid message schema
  - Photo not found in S3
  - Database constraint violations
  - Malformed JSON

### Testing Standards
- **Unit Tests**:
  - Message schema validation
  - Retry logic with exponential backoff
  - Error classification (transient vs. permanent)
  - State transitions
- **Integration Tests**:
  - Full message flow: publish → consume → process → update status
  - Retry behavior with network failures (mocked)
  - DLQ routing for permanent failures
  - Worker concurrency with multiple workers
- **End-to-End Tests**:
  - Photo upload → message published → worker processes → status updated
  - Simulate SQS delays and long polling
- **Test Coverage**: >80% for queue and worker logic
- **Test Framework**: Pytest (Python) or Jest (Node.js)

---

## Tasks / Subtasks

- [ ] Create SQS Queue Infrastructure (AC: 1, 2)
  - [ ] Add SQS queue definitions to Terraform (Story 1.2 infrastructure)
  - [ ] Create 3 priority queues: high, normal, low in terraform
  - [ ] Configure queue settings: message retention (14 days), visibility timeout (300s)
  - [ ] Enable long polling (20-second wait time)
  - [ ] Create Dead Letter Queues for each priority level
  - [ ] Configure redrive policy: max receive count = 4
  - [ ] Test queue creation and connectivity with boto3

- [ ] Configure Message Schema and Validation (AC: 7)
  - [ ] Create message schema definition (Pydantic/JSON Schema)
  - [ ] Implement schema validator with comprehensive field checks
  - [ ] Test schema validation with valid and invalid messages
  - [ ] Document required and optional fields in message schema
  - [ ] Create example message payloads for documentation

- [ ] Implement Worker Service Base (AC: 5)
  - [ ] Create `/backend/src/workers/photo_processor.py` or `photoProcessor.ts`
  - [ ] Create SQS client initialization with boto3/AWS SDK
  - [ ] Implement message consumer that polls SQS queue
  - [ ] Create message acknowledgment (delete from queue) after successful processing
  - [ ] Add logging for message receipt and processing
  - [ ] Test worker startup and queue polling

- [ ] Implement Retry and Backoff Logic (AC: 3)
  - [ ] Create retry manager with exponential backoff: 1s, 2s, 4s
  - [ ] Add jitter to backoff times to prevent thundering herd
  - [ ] Implement max retry count (4 attempts total)
  - [ ] Move messages to DLQ after max retries
  - [ ] Log retry attempts with context
  - [ ] Test retry logic with simulated failures

- [ ] Implement Processing State Management (AC: 6)
  - [ ] Create database migration for `processing_jobs` table
  - [ ] Create ORM model for processing job tracking
  - [ ] Implement functions: create_job(), update_status(), mark_complete(), mark_failed()
  - [ ] Store job start time and processing duration
  - [ ] Create function to retrieve job status by photo_id
  - [ ] Test state transitions and database updates

- [ ] Implement Worker Message Processing (AC: 5, 6)
  - [ ] Create message handler function with comprehensive error handling
  - [ ] Parse and validate message schema
  - [ ] Extract photo_id, s3_url, detection_types from message
  - [ ] Update processing status: queued → received → processing
  - [ ] Trigger next processing step (detection pipeline - placeholder for now)
  - [ ] Handle processing errors with appropriate status updates
  - [ ] Test full message flow with sample messages

- [ ] Create Dead Letter Queue Handler (AC: 4)
  - [ ] Create function to monitor Dead Letter Queue
  - [ ] Implement DLQ message logging and alerting
  - [ ] Create manual reprocessing capability for recoverable failures
  - [ ] Store DLQ messages for analysis and debugging
  - [ ] Create endpoint to inspect DLQ messages
  - [ ] Test message routing to DLQ after max retries

- [ ] Implement Worker Monitoring and Metrics (AC: 8)
  - [ ] Create custom CloudWatch metrics for processing
  - [ ] Track: messages processed, failures, processing time, queue depth
  - [ ] Create worker health check endpoint
  - [ ] Implement logging with structured JSON format
  - [ ] Add correlation IDs for tracing message through system
  - [ ] Test metrics publishing and CloudWatch dashboard

- [ ] Configure CloudWatch Alarms (AC: 8)
  - [ ] Create alarm for queue depth > 1000 (trigger auto-scaling)
  - [ ] Create alarm for DLQ messages > 10
  - [ ] Create alarm for processing time > 5 minutes
  - [ ] Create alarm for worker failure rate > 5%
  - [ ] Configure SNS notifications for critical alarms
  - [ ] Test alarms with manual threshold breaches

- [ ] Implement Docker Compose RabbitMQ Setup (AC: 1, 2)
  - [ ] Add RabbitMQ service to `docker-compose.yml`
  - [ ] Configure RabbitMQ image with management plugin
  - [ ] Set up queues and exchanges via init script
  - [ ] Create development default credentials
  - [ ] Document RabbitMQ management UI access (localhost:15672)
  - [ ] Test RabbitMQ connectivity from worker service

- [ ] Create Comprehensive Tests (AC: 1-8)
  - [ ] Unit tests for message schema validation
  - [ ] Unit tests for retry logic with various failure scenarios
  - [ ] Unit tests for state transitions
  - [ ] Integration tests with mocked SQS (moto library)
  - [ ] Integration tests with message processing pipeline
  - [ ] Integration tests for DLQ routing
  - [ ] Test worker with concurrent message processing
  - [ ] Test error handling and recovery
  - [ ] Achieve >80% test coverage

- [ ] Create Documentation (AC: 1-8)
  - [ ] Document message schema with field descriptions
  - [ ] Document worker setup and configuration
  - [ ] Document retry strategy and failure scenarios
  - [ ] Create troubleshooting guide for common issues
  - [ ] Document monitoring and alerting setup
  - [ ] Document manual DLQ message reprocessing
  - [ ] Create operational runbook for queue management

---

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-11-17 | 1.0 | Initial story creation for message queue and async processing | Bob (Scrum Master) |

---

## Dev Agent Record

*To be populated by development agent during implementation*

### Agent Model Used
TBD

### Debug Log References
TBD

### Completion Notes
TBD

### File List
TBD

---

## QA Results

*To be populated by QA agent after development*
